{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Data and Save\n",
    "Data was obtained from http://compprag.christopherpotts.net/swda.html which is the switchboard data. It is processed and has pos tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from builtins  import any as b_any\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import pickle\n",
    "import random \n",
    "PATH = \"E:\\CompLing575\\ProcessedData\\swda\\swda\"\n",
    "FileList = [os.path.join(dp, f) for dp, dn, filenames in os.walk(PATH) for f in filenames if os.path.splitext(f)[1] == '.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data  comes with a script to load it. I ported the script imported in the next line to python 3.4. \n",
    "I am loading the data using the given function Transcript and I also create a set of n grams to go with it. \n",
    "I will save it to a pickle file which can be loaded later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from swda import Transcript\n",
    "Transcript_list = []\n",
    "\n",
    "for f in FileList:\n",
    "    trans = Transcript(f, \"E:\\CompLing575\\ProcessedData\\swda\\swda\\swda-metadata\")\n",
    "    utts=trans.utterances\n",
    "    \n",
    "    for utt in utts:\n",
    "        pos_text = utt.pos.replace(\"[\",\" \")\n",
    "        pos_text = pos_text.replace(\"]\", \" \")\n",
    "        pos_list = list(filter(None,pos_text.split(' ')))\n",
    "        pos_words = list(word[0:word.index('/')] for word in pos_list)\n",
    "        bigrams=list(ngrams(pos_words,2))\n",
    "        trigrams = list (ngrams(pos_words,3))\n",
    "        pos_bigrams=list(ngrams(pos_list,2))\n",
    "        pos_trigrams = list (ngrams(pos_list,3))\n",
    "\n",
    "        data = [utt.act_tag,pos_words,bigrams,trigrams,pos_list,pos_bigrams,pos_trigrams ]\n",
    "        Transcript_list.append(data)\n",
    "\n",
    "outputFile = open('E:\\CompLing575\\ProcessedData\\swda\\Transcript_list.pkl', 'wb')\n",
    "pickle.dump(Transcript_list, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Separate into 2 classes and create test and training set. Also create vocab from training set\n",
    "StatementTag = ['sd','sv']\n",
    "Statement = []\n",
    "Rest = []\n",
    "st_count = 0\n",
    "r_count = 0\n",
    "for utt in Transcript_list:\n",
    "    if b_any(i in utt[0] for i in StatementTag):\n",
    "        tag = \"1\"\n",
    "        data = [st_count,tag,utt[1:]]\n",
    "        st_count +=1\n",
    "        Statement.append(data)\n",
    "    else:\n",
    "        tag = \"0\"\n",
    "        data = [r_count,tag,utt[1:]]\n",
    "        r_count+=1\n",
    "        Rest.append(data)\n",
    "\n",
    "StateTestId = random.sample(range(0,len(Statement)),  int(math.floor(len(Statement)*0.3)))\n",
    "RestTestId = random.sample(range(0,len(Rest)),  int(math.floor(len(Rest)*0.3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate training set with vocab and create test set\n",
    "Vocab = dict()\n",
    "Vocab_pos = dict()\n",
    "training = []\n",
    "test = []\n",
    "for st in Statement:\n",
    "    if (st[0] not in StateTestId):\n",
    "        training.append(st[1:])\n",
    "        for word in st[2][0]:\n",
    "            Vocab[word] = Vocab.get(word, 0) + 1\n",
    "        for bi in st[2][1]:\n",
    "            word = '_'.join(bi)\n",
    "            Vocab[word] = Vocab.get(word, 0) + 1\n",
    "        for tri in st[2][2]:\n",
    "            word = '_'.join(tri)\n",
    "            Vocab[word] = Vocab.get(word, 0) + 1\n",
    "        for word in st[2][3]:\n",
    "            Vocab_pos[word] = Vocab_pos.get(word, 0) + 1\n",
    "        for bi in st[2][4]:\n",
    "            word = '_'.join(bi)\n",
    "            Vocab_pos[word] = Vocab_pos.get(word, 0) + 1\n",
    "        for tri in st[2][5]:\n",
    "            word = '_'.join(tri)\n",
    "            Vocab_pos[word] = Vocab_pos.get(word, 0) + 1\n",
    "    else:\n",
    "        test.append(st[1:])\n",
    "for rt in Rest:\n",
    "    if (rt[0] not in RestTestId):\n",
    "        training.append(rt[1:])\n",
    "        for word in rt[2][0]:\n",
    "            Vocab[word] = Vocab.get(word, 0) + 1\n",
    "        for bi in rt[2][1]:\n",
    "            word = '_'.join(bi)\n",
    "            Vocab[word] = Vocab.get(word, 0) + 1\n",
    "        for tri in rt[2][2]:\n",
    "            word = '_'.join(tri)\n",
    "            Vocab[word] = Vocab.get(word, 0) + 1\n",
    "        for word in rt[2][3]:\n",
    "            Vocab_pos[word] = Vocab_pos.get(word, 0) + 1\n",
    "        for bi in rt[2][4]:\n",
    "            word = '_'.join(bi)\n",
    "            Vocab_pos[word] = Vocab_pos.get(word, 0) + 1\n",
    "        for tri in rt[2][5]:\n",
    "            word = '_'.join(tri)\n",
    "            Vocab_pos[word] = Vocab_pos.get(word, 0) + 1\n",
    "    else:\n",
    "        test.append(rt[1:])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET STATISTICS: \n",
      "RestTestid Count:36016\n",
      "Rest data Count:120055\n",
      "StatementTestid Count:30468\n",
      "Statement data Count:101561\n",
      "Total utterance Count:221616\n",
      "Percentage statement Count:45.82746733087864\n"
     ]
    }
   ],
   "source": [
    "print (\"DATASET STATISTICS: \")\n",
    "print(\"RestTestid Count:\"+str(len(RestTestId)))\n",
    "print(\"Rest data Count:\"+str(len(Rest)))\n",
    "print(\"StatementTestid Count:\"+str(len(StateTestId)))\n",
    "print(\"Statement data Count:\"+str(len(Statement)))\n",
    "print(\"Total utterance Count:\"+str(len(Statement)+len(Rest)))\n",
    "print(\"Percentage statement Count:\"+str((len(Statement)*100)/ (len(Statement)+len(Rest))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to save all the files for future processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile = open('E:\\CompLing575\\ProcessedData\\swda\\TrainingData.pkl', 'wb')\n",
    "pickle.dump(training, outputFile)\n",
    "outputFile.close()\n",
    "outputFile = open('E:\\CompLing575\\ProcessedData\\swda\\TestData.pkl', 'wb')\n",
    "pickle.dump(test, outputFile)\n",
    "outputFile.close()\n",
    "outputFile = open('E:\\CompLing575\\ProcessedData\\swda\\Vocab.pkl', 'wb')\n",
    "pickle.dump(Vocab, outputFile)\n",
    "outputFile.close()\n",
    "outputFile = open('E:\\CompLing575\\ProcessedData\\swda\\Vocab_pos.pkl', 'wb')\n",
    "pickle.dump(Vocab_pos, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
